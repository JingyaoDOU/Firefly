{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sending data to Firefly through Flask\n",
    "\n",
    "*Working from customReader.ipynb*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from firefly_api.reader import SimpleReader\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg --proc bg_proc\n",
    "python /Users/agurvich/research/repos/Firefly/FireflyFlaskApp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Firefly in an IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1227effd0>"
      ],
      "text/html": "\n        <iframe\n            width=\"1000\"\n            height=\"500\"\n            src=\"http://localhost:5000\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "url = \"http://localhost:5000\"\n",
    "IFrame(url, width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some data, and get the Firefly dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output to dummy.hdf5\noutput to dummy.csv\nOpening 1 files and 1 particle types...\nJSONdir is None, defaulting to /Users/agurvich/research/repos/Firefly/static/data/Data\noutputting: \nParticleGroup: dummy\nContains 7999 particles (7999 after decimation) and 0 tracked fields\nOpening 1 files and 1 particle types...\nJSONdir is None, defaulting to /Users/agurvich/research/repos/Firefly/static/data/Data\noutputting: \nParticleGroup: group1\nContains 8000 particles (8000 after decimation) and 0 tracked fields\n"
     ]
    }
   ],
   "source": [
    "## Here I'll create a grid of points in the shape of a cube\n",
    "my_coords = np.linspace(-10,10,20)\n",
    "xs,ys,zs = np.meshgrid(my_coords,my_coords,my_coords)\n",
    "xs,ys,zs = xs.flatten(),ys.flatten(),zs.flatten()\n",
    "coords = np.array([xs,ys,zs]).T\n",
    "\n",
    "with h5py.File(\"dummy.hdf5\",'w') as handle:\n",
    "    group = handle.create_group('group1')\n",
    "    group['Coordinates'] = coords\n",
    "print(\"output to dummy.hdf5\")\n",
    "\n",
    "np.savetxt('dummy.csv',coords)\n",
    "print(\"output to dummy.csv\")\n",
    "\n",
    "## test out the csv\n",
    "simple = SimpleReader(\"dummy.csv\",write_jsons_to_disk=False,extension='.csv')\n",
    "\n",
    "## test out the hdf5\n",
    "simple = SimpleReader(\"dummy.hdf5\",write_jsons_to_disk=False,extension='.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send this data to the Flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "outputting: \nParticleGroup: group1\nContains 8000 particles (8000 after decimation) and 0 tracked fields\n"
     ]
    }
   ],
   "source": [
    "JSON_array = simple.dumpToJSON(write_jsons_to_disk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Opening 1 files and 6 particle types...\n",
      "JSONdir is None, defaulting to /Users/agurvich/research/repos/Firefly/static/data/Data\n",
      "outputting: \n",
      "ParticleGroup: PartType0\n",
      "Contains 1066668 particles (10666 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType1\n",
      "Contains 5860827 particles (58608 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType2\n",
      "Contains 6175377 particles (61753 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType3\n",
      "Contains 7732157 particles (77321 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType4\n",
      "Contains 514118 particles (5141 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType5\n",
      "Contains 1 particles (1 after decimation) and 0 tracked fields\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Opening 1 files and 6 particle types...\n",
      "JSONdir is None, defaulting to /Users/agurvich/research/repos/Firefly/static/data/Data\n",
      "outputting: \n",
      "ParticleGroup: PartType0\n",
      "Contains 1066668 particles (10666 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType1\n",
      "Contains 5860827 particles (58608 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType2\n",
      "Contains 6175377 particles (61753 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType3\n",
      "Contains 7732157 particles (77321 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType4\n",
      "Contains 514118 particles (5141 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType5\n",
      "Contains 1 particles (1 after decimation) and 0 tracked fields\n",
      "> \u001b[0;32m/Users/agurvich/research/repos/Firefly/dataReader/firefly_api/reader.py\u001b[0m(284)\u001b[0;36mdumpToJSON\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    282 \u001b[0;31m            \u001b[0;31m##  we want to send dataViaFlask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    283 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 284 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSON_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    285 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    286 \u001b[0;31m            \u001b[0;31m## we wrote all the little JSONs to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "9307657\n"
     ]
    }
   ],
   "source": [
    "reader = SimpleReader(\"/users/agurvich/research/snaps/isolated_disks/Control_G4_20/snapdir_050\",\n",
    "    write_jsons_to_disk=False,\n",
    "    decimation_factor=100)\n",
    "print(len(reader.JSON))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "outputting: \n",
      "ParticleGroup: PartType0\n",
      "Contains 1066668 particles (10666 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType1\n",
      "Contains 5860827 particles (58608 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType2\n",
      "Contains 6175377 particles (61753 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType3\n",
      "Contains 7732157 particles (77321 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType4\n",
      "Contains 514118 particles (5141 after decimation) and 0 tracked fields\n",
      "outputting: \n",
      "ParticleGroup: PartType5\n",
      "Contains 1 particles (1 after decimation) and 0 tracked fields\n"
     ]
    }
   ],
   "source": [
    "reader.dumpToJSON(write_jsons_to_disk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"Coordinates\":[[-204.3753051758,134.370010376,-12'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "JSON_arrays[2][1][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20.48990797996521 s elapsed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "init = time.time()\n",
    "foo = reader.outputToDict(JSON=True)\n",
    "print(time.time()-init,'s elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"parts\":{\"PartType0\":{\"Coordinates\":[[-8.290719986,6.6803894043,-12.6101322174],[-7.0236144066,7.12'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "foo[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This sends the data.  (Then look back above to see it.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "posting...data posted!\n"
     ]
    }
   ],
   "source": [
    "simple.sendDataViaFlask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "## alternatively, make the request yourself (literally just copying the code from sendDataViaFlask())\n",
    "port = 5000\n",
    "outputDict = simple.outputToDict(json_friendly=True)\n",
    "requests.post(f'http://localhost:{port:d}/data_input',json=json.dumps(outputDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To kill the Firefly server.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kill: 29931: No such process\n"
     ]
    }
   ],
   "source": [
    "# uncomment the next line\n",
    "! ps aux | grep Firefly | awk '{print $2}' | xargs kill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('firefly': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "f534f06944b7101ee418c38585e628955b3f3d62b78942ee260617f70aa005fb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}